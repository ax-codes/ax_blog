---
title: Scrapy学习篇(四)-第一个项目
date: 2018/01/19 20:00:00
categories: 技术
toc: True
tags: [python]
---

### 前言

上一篇我们已经安装好了 Scrapy 现在我们将开始使用 Scrapy 抓取数据
我们将爬取 Scrapy 官网文档地址的数据,链接:https://doc.scrapy.org/en/latest/

### 创建项目

#### startproject

进入到我们需要将项目文件放置的地址执行:

```shell
scrapy startproject mySpider
```

注意:mySpider 是我们的项目名称

### 项目结构

执行完创建命令后该路径下会生成 mySpider 文件夹,目录结构:

```
mySpider/
    scrapy.cfg
    mySpider/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
```

文件的作用:
scrapy.cfg: 项目的配置文件.
mySpider/: 项目的 Python 模块,将会从这里引用代码.
mySpider/middlewares.py：中间件
mySpider/items.py: 项目的目标文件.(一般直接用再爬虫代码里生成对象)
mySpider/pipelines.py: 项目的管道文件.(必须在 setting 里注册,然后在爬虫代码里返回 item 才会触发)
mySpider/settings.py: 项目的设置文件.
mySpider/spiders/: 存储爬虫代码目录.

### 创建爬虫文件

可以在项目目录里使用命令创建,也可以直接在"mySpider/spiders/"目录下手动创建,这里我们使用命令创建,因为这样会自动帮我们生成 spider 模板代码:

```shell
scrapy genspider test "doc.scrapy.org/en/latest"
```

注意:doc.scrapy.org/en/latest 这里不用加上 http 或者 https,模板会自动加上

执行完上面创建爬虫的命令后,会在"mySpider/spiders/"里生成一个"test.py"文件,内容为:

```python
# -*- coding: utf-8 -*-
import scrapy


class TestSpider(scrapy.Spider):
    name = 'test'
    allowed_domains = ['doc.scrapy.org/en/latest']
    start_urls = ['http://doc.scrapy.org/en/latest/']

    def parse(self, response):
        pass

```

内容解释:
name = "" :这个爬虫的识别名称,必须是唯一的,在不同的爬虫必须定义不同的名字
allow_domains = [] 是搜索的域名范围,也就是爬虫的约束区域,规定爬虫只爬取这个域名下的网页,不存在的 URL 会被忽略
start_urls = () :爬取的 URL 元祖/列表.爬虫从这里开始抓取数据,所以,第一次下载的数据将会从这些 urls 开始.其他子 URL 将会从这些起始 URL 中继承性生成
parse(self, response) :解析的方法,每个初始 URL 完成下载后将被调用,调用的时候传入从每一个 URL 传回的 Response 对象来作为唯一参数,主要作用如下:
负责解析返回的网页数据(response.body),提取结构化数据(生成 item)
生成需要下一页的 URL 请求.
将 start_urls 的值修改为需要爬取的第一个 url,如果我们需要爬取该域名下的其他界面,可以把连接添加到 start_urls

### 运行爬虫

将"mySpider/spiders/test.py"的内容修改为

```python
# -*- coding: utf-8 -*-
import scrapy

# 以下三行是在 Python2.x版本中解决乱码问题，Python3.x 版本的可以去掉
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

class TestSpider(scrapy.Spider):
    name = 'test'
    allowed_domains = ['doc.scrapy.org/en/latest']
    start_urls = ['http://doc.scrapy.org/en/latest/']

    def parse(self, response):
        filename = "teacher.html"
        open(filename, 'w').write(response.body)#保存body到teacher.html文件
        pass
```

---
